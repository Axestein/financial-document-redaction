{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wnsQPiXyf0M"
      },
      "outputs": [],
      "source": [
        "# Install 'transformers' for the BERT model, 'datasets' for data handling,\n",
        "# and 'evaluate' for performance metrics.\n",
        "!pip install transformers datasets evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Please upload your 'FR_NFR_Dataset.xlsx - datasetFR_NFR_full.csv' file.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Store the uploaded filename for the next cell\n",
        "if uploaded:\n",
        "    file_name = next(iter(uploaded))\n",
        "    print(f\"\\nSuccessfully uploaded '{file_name}'\")\n",
        "else:\n",
        "    print(\"\\nNo file was uploaded. Please run the cell again to upload.\")"
      ],
      "metadata": {
        "id": "3aocYDJF04Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import re\n",
        "\n",
        "if 'file_name' in locals() and file_name in uploaded:\n",
        "    try:\n",
        "        # Use pandas.read_excel() as the file is an Excel file, not a CSV.\n",
        "        # This is the definitive fix for the loading and casting errors.\n",
        "        df = pd.read_excel(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "        # Force the column names to be 'text' and 'label_str' for consistency\n",
        "        # This assumes the first column is the requirement and the second is the type.\n",
        "        df.columns = ['text', 'label_str']\n",
        "\n",
        "        # Ensure the 'text' column is of string type\n",
        "        df['text'] = df['text'].astype(str)\n",
        "\n",
        "        # Map string labels to integer IDs for the model\n",
        "        label2id = {\"FR\": 0, \"NFR\": 1}\n",
        "        # Clean the label string and then map it\n",
        "        df['label_str'] = df['label_str'].str.extract(r'(FR|NFR)', expand=False, flags=re.IGNORECASE).str.upper()\n",
        "        df['label'] = df['label_str'].map(label2id)\n",
        "\n",
        "        # Clean up and prepare the DataFrame\n",
        "        df = df.dropna(subset=['text', 'label'])\n",
        "        df['label'] = df['label'].astype(int)\n",
        "        df = df[['text', 'label']]\n",
        "\n",
        "        print(f\"Successfully loaded and cleaned {len(df)} records from the Excel file.\")\n",
        "        print(\"\\n--- Data Preview ---\")\n",
        "        print(df.head())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the Excel file: {e}\")\n",
        "        print(\"Please ensure you have uploaded the correct '.xlsx' file.\")\n",
        "else:\n",
        "    print(\"Data not loaded. Please ensure you uploaded a file in Cell 2.\")"
      ],
      "metadata": {
        "id": "7SRjY2Tg1H2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "if 'df' in locals() and not df.empty:\n",
        "    # Convert pandas DataFrame to Hugging Face Dataset\n",
        "    hg_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    # Split into training and testing sets (80% train, 20% test)\n",
        "    hg_dataset = hg_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    print(\"--- Dataset Structure ---\")\n",
        "    print(hg_dataset)\n",
        "\n",
        "    # Load the pre-trained tokenizer ('distilbert-base-uncased' is fast and effective)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    # Define the function that will tokenize the text\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Apply the tokenization to our entire dataset\n",
        "    tokenized_datasets = hg_dataset.map(tokenize_function, batched=True)\n",
        "    print(\"\\nTokenization complete.\")\n",
        "    print(\"\\n--- Sample Tokenized Record ---\")\n",
        "    print(tokenized_datasets['train'][0])\n",
        "else:\n",
        "    print(\"DataFrame 'df' not found or is empty. Please check the previous cell for errors.\")"
      ],
      "metadata": {
        "id": "2416BX-B1ht1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "if 'tokenized_datasets' in locals():\n",
        "    # Load the pre-trained DistilBERT model for sequence classification\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",\n",
        "        num_labels=2,\n",
        "        id2label={0: \"FR\", 1: \"NFR\"},\n",
        "        label2id={\"FR\": 0, \"NFR\": 1}\n",
        "    )\n",
        "\n",
        "    # Define the training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=3,              # A good starting point\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"epoch\",      # Evaluate at the end of each epoch\n",
        "        save_strategy=\"epoch\",            # Save a checkpoint after each epoch\n",
        "        load_best_model_at_end=True,      # Automatically load the best model\n",
        "    )\n",
        "\n",
        "    # Define the evaluation metric\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Create the Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"test\"],\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Start training!\n",
        "    print(\"--- Starting Model Training ---\")\n",
        "    trainer.train()\n",
        "    print(\"--- Training Complete ---\")\n",
        "else:\n",
        "    print(\"Tokenized dataset not found. Please run the previous cells successfully.\")"
      ],
      "metadata": {
        "id": "6GZaz8rO1kht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "if 'trainer' in locals():\n",
        "    # The trainer automatically saves the best performing model.\n",
        "    # We can load it directly into a pipeline.\n",
        "    best_model_path = trainer.state.best_model_checkpoint\n",
        "    print(f\"Loading best model from: {best_model_path}\")\n",
        "\n",
        "    classifier = pipeline(\"text-classification\", model=best_model_path, tokenizer=tokenizer)\n",
        "\n",
        "    # Example 1: Should be Functional (FR)\n",
        "    text_fr = \"The system shall email a confirmation link to the user upon registration.\"\n",
        "    result_fr = classifier(text_fr)\n",
        "    print(f\"\\nText: '{text_fr}'\")\n",
        "    print(f\"Prediction: {result_fr[0]['label']} (Score: {result_fr[0]['score']:.4f})\")\n",
        "\n",
        "    # Example 2: Should be Non-Functional (NFR)\n",
        "    text_nfr = \"The user interface should be intuitive and require minimal training.\"\n",
        "    result_nfr = classifier(text_nfr)\n",
        "    print(f\"\\nText: '{text_nfr}'\")\n",
        "    print(f\"Prediction: {result_nfr[0]['label']} (Score: {result_nfr[0]['score']:.4f})\")\n",
        "else:\n",
        "    print(\"Trainer not found. Please complete the training in the previous cell.\")"
      ],
      "metadata": {
        "id": "6BY6giKY1mq1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}