{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wN1qxQ6NXtn"
      },
      "outputs": [],
      "source": [
        "# Install transformers for the BERT model, datasets for data handling,\n",
        "# and seqeval for NER-specific evaluation metrics.\n",
        "!pip install transformers datasets evaluate seqeval -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# 1. Define all unique NER tags your model should learn.\n",
        "labels_list = [\n",
        "    \"O\",\n",
        "    \"B-NAME\", \"I-NAME\",\n",
        "    \"B-ID\",\n",
        "    \"B-ADDRESS\", \"I-ADDRESS\"\n",
        "]\n",
        "\n",
        "# 2. Create mappings between the string labels and integer IDs.\n",
        "label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "id2label = {i: label for i, label in enumerate(labels_list)}\n",
        "\n",
        "print(\"--- Label to ID Mapping ---\")\n",
        "print(label2id)\n",
        "\n",
        "# 3. Create your raw dataset. This is the smaller, original dataset.\n",
        "raw_data = {\n",
        "    'id': ['0', '1', '2', '3'],\n",
        "    'tokens': [\n",
        "        [\"My\", \"name\", \"is\", \"Priya\", \"Sharma\", \".\"],\n",
        "        [\"Please\", \"use\", \"ID\", \"number\", \"AX-451-22\", \".\"],\n",
        "        [\"He\", \"lives\", \"at\", \"123\", \"Main\", \"St\", \".\"],\n",
        "        [\"Forward\", \"mail\", \"for\", \"John\", \"Doe\", \"to\", \"123\", \"Main\", \"St\", \".\"]\n",
        "    ],\n",
        "    'ner_tags_str': [\n",
        "        [\"O\", \"O\", \"O\", \"B-NAME\", \"I-NAME\", \"O\"],\n",
        "        [\"O\", \"O\", \"O\", \"O\", \"B-ID\", \"O\"],\n",
        "        [\"O\", \"O\", \"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"I-ADDRESS\", \"O\"],\n",
        "        [\"O\", \"O\", \"O\", \"B-NAME\", \"I-NAME\", \"O\", \"B-ADDRESS\", \"I-ADDRESS\", \"I-ADDRESS\", \"O\"]\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "# 4. Convert the string tags to their integer IDs.\n",
        "raw_data['ner_tags'] = [\n",
        "    [label2id[tag] for tag in tags] for tags in raw_data['ner_tags_str']\n",
        "]\n",
        "\n",
        "# 5. Convert the Python dictionary into a Hugging Face Dataset object.\n",
        "dataset = Dataset.from_dict(raw_data)\n",
        "\n",
        "print(\"\\n--- Sample Dataset Record ---\")\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "id": "T3wEpSEmOntK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer. 'bert-base-cased' is a robust choice\n",
        "# as it respects capitalization, which can be important for names.\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    \"\"\"\n",
        "    This function tokenizes text and aligns the NER labels with the\n",
        "    new subword tokens.\n",
        "    \"\"\"\n",
        "    # Tokenize the words, respecting the pre-split format.\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    # For each sentence in the batch...\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        # Get the word IDs for each token.\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        # For each token in the sentence...\n",
        "        for word_idx in word_ids:\n",
        "            # If it's a special token ([CLS], [SEP]), assign -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # If it's a new word, assign its actual label.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # If it's a subsequent subword of the same word, assign -100.\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    # Add the aligned labels to our tokenized inputs.\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ],
      "metadata": {
        "id": "gjt6FY3UOrQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the .map() method to apply the function to every record in the dataset.\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# Remove original columns that the model doesn't need for training.\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\n",
        "    'tokens', 'ner_tags_str', 'ner_tags', 'id'\n",
        "])\n",
        "\n",
        "print(\"\\n--- Tokenized and Aligned Record ---\")\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokenized_dataset[0]['input_ids']))\n",
        "print(\"Labels:\", tokenized_dataset[0]['labels'])"
      ],
      "metadata": {
        "id": "gbKSDCNvO7an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "# A Data Collator creates batches of data. This one will dynamically pad\n",
        "# sentences to the same length for every batch.\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# Load the seqeval metric, which is the standard for NER tasks.\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    \"\"\"\n",
        "    This function computes precision, recall, F1, and accuracy for the NER task.\n",
        "    \"\"\"\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Convert IDs back to string labels, removing the -100 ignored indices.\n",
        "    true_predictions = [\n",
        "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # Compute metrics using seqeval.\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "# Load the pre-trained BERT model for token classification.\n",
        "# Pass our custom label mappings to it.\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=len(labels_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "GOfM2yV1O_Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where the model checkpoints will be saved.\n",
        "model_output_dir = \"/content/drive/MyDrive/bert_pii_ner_model\"\n",
        "\n",
        "# Define the directory where the model checkpoints will be saved.\n",
        "model_output_dir = \"/content/drive/MyDrive/bert_pii_ner_model\"\n",
        "\n",
        "# Define the training hyperparameters using TrainingArguments.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_output_dir,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,  # Increase epochs for better performance on real data\n",
        "    weight_decay=0.01,\n",
        "    # NOTE: The argument names below were changed to fix a common versioning issue.\n",
        "    # Older versions of the transformers library used these names.\n",
        "    eval_strategy=\"epoch\", # Evaluate performance at the end of each epoch\n",
        "    save_strategy=\"epoch\",      # Save a model checkpoint at the end of each epoch\n",
        "    load_best_model_at_end=True, # Load the best model found during training\n",
        ")\n",
        "\n",
        "# Initialize the Trainer object.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset, # The dataset is small, so we use it for both\n",
        "    eval_dataset=tokenized_dataset,  # In practice, create a separate validation set\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start the training process!\n",
        "print(\"--- Starting Model Training ---\")\n",
        "trainer.train()\n",
        "print(\"--- Training Complete ---\")\n",
        "print(\"--- Training Complete ---\")"
      ],
      "metadata": {
        "id": "ZpMAq3-bPKKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a path for the final, ready-to-use model.\n",
        "final_model_path = f\"{model_output_dir}/final_model\"\n",
        "trainer.save_model(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "print(f\"Final model and tokenizer saved to: {final_model_path}\")"
      ],
      "metadata": {
        "id": "NY15RHQ9SPVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the inference pipeline with your custom model.\n",
        "ner_pipeline = pipeline(\"ner\", model=final_model_path, tokenizer=final_model_path)\n",
        "\n",
        "# Test the pipeline on new text.\n",
        "text = \"Please send the documents for Jane Doe to 456 Park Ave. Her reference is GZ-123-45.\"\n",
        "\n",
        "# The `aggregation_strategy` groups subword tokens back into single entities.\n",
        "results = ner_pipeline(text, aggregation_strategy=\"simple\")\n",
        "\n",
        "print(\"\\n--- Inference Results ---\")\n",
        "for entity in results:\n",
        "  print(f\"Entity: {entity['word']}\\nGroup: {entity['entity_group']}\\nScore: {entity['score']:.4f}\\n\")"
      ],
      "metadata": {
        "id": "HaOvft-pSwQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}